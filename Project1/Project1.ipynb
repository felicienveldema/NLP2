{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from random import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "def read_data(english_file, french_file):\n",
    "    english_sentences = []\n",
    "    french_sentences = []\n",
    "    with open(english_file, 'r', encoding='utf8') as engf, open(french_file, 'r', encoding='utf8') as fref:\n",
    "        for line in engf:\n",
    "            english_sentences.append([\"NULL\"] + line.split())\n",
    "        for line in fref:\n",
    "            french_sentences.append(line.split())\n",
    "    assert len(english_sentences) == len(french_sentences), 'data mismatch'\n",
    "    return list(zip(english_sentences, french_sentences))\n",
    "\n",
    "training_data = read_data(english_train, french_train)\n",
    "validation_data = read_data(english_val, french_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## IBM Model 1: Generative Process\n",
    "Step 1: Pick an alignmet $a$ with probability  p(a|e,m) = $\\frac{1}{(l+1)^m}$\n",
    "\n",
    "Step 2: pick the French words with probability\n",
    "\n",
    "p(f|a,e,m) = $\\prod^m_{j=1} t(f_j | e_aj )$\n",
    "\n",
    "\n",
    "\n",
    "P(f,a|e, m ) = p(a|e,m) $\\times$ p(f|a,e,m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def align_all(data, translate_dict, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = align(english_words, french_words, translate_dict, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "    \n",
    "def align(english_words, french_words, translate_dict, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict and fword in translate_dict[eword]:\n",
    "                prob = translate_dict[eword][fword]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "        # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def log_likelihood(data, translate_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood_nonorm = 0\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = align(e, f, translate_dict, True)\n",
    "        prob = 0\n",
    "        prob2 = 0\n",
    "        for j, i in alignment:\n",
    "            prob += math.log(translate_dict[e[j]][f[i-1]]) + -len(f) * np.log(len(e) + 1)\n",
    "            prob2 += math.log(translate_dict[e[j]][f[i-1]])\n",
    "        log_likelihood += prob\n",
    "        log_likelihood_nonorm += prob2\n",
    "    return log_likelihood, log_likelihood_nonorm\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    co_counts = defaultdict(Counter)\n",
    "    for e, f in data:\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    co_counts[e_word][f_word] = 1\n",
    "                else:\n",
    "                    co_counts[e_word][f_word] = random()\n",
    "    for e_word in co_counts:\n",
    "        normalization_factor = sum(list(co_counts[e_word].values()))\n",
    "        for f_word in co_counts[e_word]:\n",
    "            co_counts[e_word][f_word] = co_counts[e_word][f_word] / normalization_factor\n",
    "    return co_counts\n",
    "\n",
    "def EM_IBM1(data, validation, max_steps=20, translate_dict=None):\n",
    "    print(\"Initializing translation dictionary.\")\n",
    "    if translate_dict is None:\n",
    "        translate_dict = initialize_t(data)\n",
    "    for iteration in range(max_steps):\n",
    "        change = False\n",
    "        fname = 'iteration' + str(iteration) + '.txt'\n",
    "        counts = Counter()\n",
    "        co_counts = defaultdict(Counter)\n",
    "        \n",
    "        print(\"Expectation step {}\".format(iteration + 1))\n",
    "        for e_s, f_s in tqdm(data):\n",
    "            for f in f_s:\n",
    "                sum_of_probs = sum([translate_dict[e2][f] for e2 in e_s])\n",
    "                for e in e_s:\n",
    "                    delta = translate_dict[e][f] / sum_of_probs\n",
    "                    co_counts[e][f] += delta\n",
    "                    counts[e] += delta\n",
    "\n",
    "        print(\"Maximisation step {}\".format(iteration + 1))\n",
    "        for e in co_counts:\n",
    "            for f in co_counts[e]:\n",
    "                new_value = co_counts[e][f] / counts[e]\n",
    "                if abs(translate_dict[e][f] - new_value) > 1e-5:\n",
    "                    change = True\n",
    "                translate_dict[e][f] = new_value\n",
    "        if not change:\n",
    "            break\n",
    "        #writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, fname)\n",
    "        ll, ll2 = log_likelihood(data, translate_dict)\n",
    "        print(ll, ll2)\n",
    "        test(\"\", alignments)\n",
    "    return translate_dict\n",
    "\n",
    "translate_dict = EM_IBM1(training_data, validation_data, 10, translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation_probs(english_word, transdict):\n",
    "    results = []\n",
    "    for french_word in translate_dict[english_word]:\n",
    "        results.append((translate_dict[english_word][french_word], french_word))\n",
    "    results.sort(reverse=True)\n",
    "    for r in results[:20]:\n",
    "        print(r)\n",
    "        \n",
    "print_translation_probs('commissioners', translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AER Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path, personal_sets=None):\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments('validation/dev.wa.nonullalign')\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    print(metric.aer())\n",
    "    return metric.aer()\n",
    "\n",
    "# test(fname)\n",
    "#hardcoded AER test for 5 iterations\n",
    "for i in range(10):\n",
    "    test('iteration'+str(i)+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_IBM2(english, french, initial_translation_estimate, max_steps=3):\n",
    "    translate_dict = initial_translation_estimate\n",
    "    jump_dict = {}\n",
    "    for _ in range(max_steps):\n",
    "        counts = defaultdict(int)\n",
    "        co_counts = defaultdict(int)\n",
    "        jump_counts = defaultdict(int)\n",
    "        pos_counts = 0\n",
    "        for e_s,f_s in tqdm(zip(english, french)):\n",
    "            m = len(f_s)\n",
    "            l = len(e_s)\n",
    "            for i, f in enumerate(f_s):\n",
    "                sum_of_probs = 0\n",
    "                for j, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 0.1)\n",
    "                    translate_prob = translate_dict.get((e,f), 0.1)\n",
    "                    sum_of_probs += jump_prob * translate_prob\n",
    "                for j, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 0.1)\n",
    "                    translate_prob = translate_dict.get((e,f), 0.1)\n",
    "                    prob = jump_prob * translate_prob / sum_of_probs\n",
    "                    co_counts[(e,f)] += prob\n",
    "                    counts[e] += prob\n",
    "                    jump_counts[get_jump(j,i,l,m)] += prob\n",
    "                    pos_counts += prob\n",
    "        for e,f in co_counts:\n",
    "            translate_dict[(e,f)] = co_counts[(e, f)] / counts[e]\n",
    "        for jump in jump_counts:\n",
    "            jump_dict[jump] = jump_counts[jump] / pos_counts\n",
    "    return translate_dict, jump_dict\n",
    "\n",
    "def get_jump(eng_pos, fre_pos, eng_len, fre_len):\n",
    "    equivalent_pos = round(fre_pos * eng_len / fre_len)\n",
    "    return eng_pos - equivalent_pos\n",
    "\n",
    "ibm2_transdict, ibm2_jumpdict = EM_IBM2(english_sentences, french_sentences, translate_dict, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_translation_probs('agriculture', ibm2_transdict)\n",
    "jumps = np.array(sorted(ibm2_jumpdict.items()))\n",
    "plt.plot(jumps[65:85,0], jumps[65:85,1])\n",
    "plt.show()\n",
    "print(sum(jumps[:,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
