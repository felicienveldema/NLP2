{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from random import random\n",
    "from scipy import special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the -UNK- token to the data.\n",
      "English data complete.\n",
      "French data complete\n",
      "Adding the -UNK- token to the data.\n",
      "English data complete.\n",
      "French data complete\n"
     ]
    }
   ],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "def add_unk(original_sentences, threshold, ext_sentences=None):\n",
    "    #use original sentences and unk low frequency words\n",
    "    if ext_sentences is None: \n",
    "        counts = Counter([item for sublist in original_sentences for item in sublist])\n",
    "        unk_words = list({k:counts[k] for k in counts if counts[k] <= threshold})\n",
    "    #use external sentences and unk unknown words\n",
    "    else:\n",
    "        counts = Counter([item for sublist in ext_sentences for item in sublist])\n",
    "        unk_words = list({k:counts[k] for k in counts})\n",
    "    for line, sentence in enumerate(tqdm(original_sentences)):\n",
    "        for index, word in enumerate(sentence):\n",
    "            if word in unk_words and ext_sentences is None:\n",
    "                original_sentences[line][index] = '-UNK-'\n",
    "            elif word not in unk_words and ext_sentences is not None:\n",
    "                original_sentences[line][index] = '-UNK-'\n",
    "    return original_sentences\n",
    "    \n",
    "\n",
    "def read_data(english_file, french_file, unk=False, threshold=1, ttype='training', eng_data=None, fre_data=None):\n",
    "    english_sentences = []\n",
    "    french_sentences = []\n",
    "    with open(english_file, 'r', encoding='utf8') as engf, open(french_file, 'r', encoding='utf8') as fref:\n",
    "        for line in engf:\n",
    "            english_sentences.append([\"NULL\"] + line.split())\n",
    "        for line in fref:\n",
    "            french_sentences.append(line.split())\n",
    "    \n",
    "    #unk cases\n",
    "    if unk:\n",
    "        print(\"Adding the -UNK- token to the data.\")\n",
    "        englishname = ttype +'_'+ str(threshold)+'_unk.e'\n",
    "        frenchname = ttype +'_'+ str(threshold)+'_unk.f'\n",
    "        #load if file is found\n",
    "        if os.path.isfile(englishname):\n",
    "            with open (englishname, 'rb') as eng:\n",
    "                english_sentences = pickle.load(eng)\n",
    "        else:\n",
    "            english_sentences = add_unk(english_sentences, threshold, eng_data)\n",
    "            with open(englishname, 'wb') as eng:\n",
    "                pickle.dump(english_sentences, eng)\n",
    "        print(\"English data complete.\")\n",
    "        \n",
    "        \n",
    "        if os.path.isfile(frenchname):\n",
    "            with open(frenchname, 'rb') as fre:\n",
    "                french_sentences = pickle.load(fre)\n",
    "        else:\n",
    "            french_sentences = add_unk(french_sentences, threshold, fre_data)\n",
    "            with open(frenchname, 'wb') as fre:\n",
    "                pickle.dump(french_sentences, fre)             \n",
    "        print(\"French data complete\")          \n",
    "        \n",
    "    assert len(english_sentences) == len(french_sentences), 'data mismatch'\n",
    "    return list(zip(english_sentences, french_sentences))\n",
    "\n",
    "training_data = read_data(english_train, french_train, True)\n",
    "ext_data = list(zip(*training_data))\n",
    "validation_data = read_data(english_val, french_val, True, ttype='validation', eng_data=ext_data[0], fre_data=ext_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AER Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49562682215743437\n",
      "0.3857421875\n",
      "0.3209169054441261\n",
      "0.32057416267942584\n",
      "0.3215311004784689\n",
      "0.323444976076555\n",
      "0.323444976076555\n",
      "0.3231357552581262\n",
      "0.3212237093690249\n",
      "0.31800766283524906\n",
      "0.31800766283524906\n",
      "0.3137065637065637\n",
      "0.3137065637065637\n",
      "0.31436837029893927\n",
      "0.3153326904532304\n",
      "0.3153326904532304\n",
      "0.3153326904532304\n",
      "0.3153326904532304\n",
      "0.3153326904532304\n",
      "0.3153326904532304\n"
     ]
    }
   ],
   "source": [
    "def test(path, personal_sets=None):\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments('validation/dev.wa.nonullalign')\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    return metric.aer()\n",
    "\n",
    "#hardcoded AER test for 5 iterations\n",
    "for i in range(20):\n",
    "    aer = test('iteration'+str(i)+'.txt')\n",
    "    print(aer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## IBM Model 1: Generative Process\n",
    "Step 1: Pick an alignmet $a$ with probability  p(a|e,m) = $\\frac{1}{(l+1)^m}$\n",
    "\n",
    "Step 2: pick the French words with probability\n",
    "\n",
    "p(f|a,e,m) = $\\prod^m_{j=1} t(f_j | e_aj )$\n",
    "\n",
    "\n",
    "\n",
    "P(f,a|e, m ) = p(a|e,m) $\\times$ p(f|a,e,m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "def align_all(data, translate_dict, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = align(english_words, french_words, translate_dict, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "    \n",
    "def align(english_words, french_words, translate_dict, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict and fword in translate_dict[eword]:\n",
    "                prob = translate_dict[eword][fword]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "        # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def log_likelihood(data, translate_dict, add_constant=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = align(e, f, translate_dict, True)\n",
    "        prob = 0\n",
    "        for j, i in alignment:\n",
    "            prob += math.log(translate_dict[e[j]][f[i-1]])\n",
    "        log_likelihood += prob\n",
    "        if add_constant:\n",
    "            log_likelihood += -len(f) * np.log(len(e) + 1)\n",
    "    return log_likelihood\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    co_counts = defaultdict(Counter)\n",
    "    for e, f in tqdm(data):\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    co_counts[e_word][f_word] = 1\n",
    "                else:\n",
    "                    co_counts[e_word][f_word] = random()\n",
    "    for e_word in co_counts:\n",
    "        normalization_factor = sum(list(co_counts[e_word].values()))\n",
    "        for f_word in co_counts[e_word]:\n",
    "            co_counts[e_word][f_word] = co_counts[e_word][f_word] / normalization_factor\n",
    "    return co_counts\n",
    "\n",
    "def EM_IBM1(data, validation, max_steps=20, translate_dict=None, epochs_trained=0):\n",
    "    print(\"Initializing translation dictionary.\")\n",
    "    if translate_dict is None:\n",
    "        translate_dict = initialize_t(data)\n",
    "    for iteration in range(epochs_trained, epochs_trained + max_steps):\n",
    "        change = False\n",
    "        fname = 'iteration' + str(iteration) + '.txt'\n",
    "        counts = Counter()\n",
    "        co_counts = defaultdict(Counter)\n",
    "        \n",
    "        print(\"Expectation step {}\".format(iteration + 1))\n",
    "        for e_s, f_s in tqdm(data):\n",
    "            for f in f_s:\n",
    "                sum_of_probs = sum([translate_dict[e2][f] for e2 in e_s])\n",
    "                for e in e_s:\n",
    "                    delta = translate_dict[e][f] / sum_of_probs\n",
    "                    co_counts[e][f] += delta\n",
    "                    counts[e] += delta\n",
    "\n",
    "        print(\"Maximisation step {}\".format(iteration + 1))\n",
    "        for e in co_counts:\n",
    "            for f in co_counts[e]:\n",
    "                new_value = co_counts[e][f] / counts[e]\n",
    "                if abs(translate_dict[e][f] - new_value) > 1e-5:\n",
    "                    change = True\n",
    "                translate_dict[e][f] = new_value\n",
    "        if not change:\n",
    "            break\n",
    "        #writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, fname)\n",
    "        ll = log_likelihood(data, translate_dict)\n",
    "        aer = test(\"\", alignments)\n",
    "        print(\"Log likelihood: {}, AER: {}\".format(ll, aer))\n",
    "        pickle.dump(translate_dict, open(\"translate_dicts/epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "    return translate_dict\n",
    "\n",
    "print(type(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing translation dictionary.\n",
      "Expectation step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:10<00:00, 537.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 1\n",
      "Log likelihood: -16445768.168144051, AER: 0.3664772727272727\n",
      "Expectation step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:08<00:00, 539.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 2\n",
      "Log likelihood: -11694690.516029175, AER: 0.3279158699808795\n",
      "Expectation step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:07<00:00, 541.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 3\n",
      "Log likelihood: -9767777.560276357, AER: 0.3209169054441261\n",
      "Expectation step 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:15<00:00, 530.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 4\n",
      "Log likelihood: -8852037.158920348, AER: 0.32057416267942584\n",
      "Expectation step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:11<00:00, 535.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 5\n",
      "Log likelihood: -8320101.917196267, AER: 0.3215311004784689\n",
      "Expectation step 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:52<00:00, 489.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 6\n",
      "Log likelihood: -7975756.922772594, AER: 0.323444976076555\n",
      "Expectation step 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [10:49<00:00, 355.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 7\n",
      "Log likelihood: -7737641.475669495, AER: 0.323444976076555\n",
      "Expectation step 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:37<00:00, 504.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 8\n",
      "Log likelihood: -7564839.842448467, AER: 0.3231357552581262\n",
      "Expectation step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:39<00:00, 502.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 9\n",
      "Log likelihood: -7434988.3206639, AER: 0.3212237093690249\n",
      "Expectation step 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:39<00:00, 502.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 10\n",
      "Log likelihood: -7334186.011778272, AER: 0.31800766283524906\n",
      "Expectation step 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:41<00:00, 501.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 11\n",
      "Log likelihood: -7254070.746853232, AER: 0.31800766283524906\n",
      "Expectation step 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:46<00:00, 495.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 12\n",
      "Log likelihood: -7189218.099847515, AER: 0.3137065637065637\n",
      "Expectation step 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:17<00:00, 527.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 13\n",
      "Log likelihood: -7135495.860541471, AER: 0.3137065637065637\n",
      "Expectation step 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:16<00:00, 529.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 14\n",
      "Log likelihood: -7090644.123121053, AER: 0.31436837029893927\n",
      "Expectation step 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:17<00:00, 528.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 15\n",
      "Log likelihood: -7052661.740290008, AER: 0.3153326904532304\n",
      "Expectation step 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:13<00:00, 533.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 16\n",
      "Log likelihood: -7020132.124201371, AER: 0.3153326904532304\n",
      "Expectation step 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:23<00:00, 521.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 17\n",
      "Log likelihood: -6992028.903662212, AER: 0.3153326904532304\n",
      "Expectation step 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:21<00:00, 523.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 18\n",
      "Log likelihood: -6967509.286361041, AER: 0.3153326904532304\n",
      "Expectation step 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:13<00:00, 533.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 19\n",
      "Log likelihood: -6945895.807228058, AER: 0.3153326904532304\n",
      "Expectation step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [07:13<00:00, 533.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 20\n",
      "Log likelihood: -6926769.577004622, AER: 0.3153326904532304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "translate_dict = EM_IBM1(training_data, validation_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_translation_probs(english_word, transdict):\n",
    "    results = []\n",
    "    for french_word in translate_dict[english_word]:\n",
    "        results.append((translate_dict[english_word][french_word], french_word))\n",
    "    results.sort(reverse=True)\n",
    "    for r in results[:20]:\n",
    "        print(r)\n",
    "        \n",
    "print_translation_probs('commissioners', translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian IBM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing translation dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                               | 0/231164 [00:00<?, ?it/s]\n",
      "  0%|                                   | 127/231164 [00:00<03:11, 1209.49it/s]\n",
      "  0%|                                   | 222/231164 [00:00<03:33, 1082.91it/s]\n",
      "  0%|                                   | 308/231164 [00:00<03:50, 1003.26it/s]\n",
      "  0%|                                   | 474/231164 [00:00<03:18, 1161.75it/s]\n",
      "  0%|                                   | 568/231164 [00:00<03:26, 1118.11it/s]\n",
      "  0%|                                   | 713/231164 [00:00<03:16, 1172.70it/s]\n",
      "  0%|▏                                 | 1117/231164 [00:00<02:26, 1573.23it/s]\n",
      "  1%|▏                                  | 1313/231164 [00:02<06:55, 553.77it/s]\n",
      "  1%|▏                                  | 1454/231164 [00:02<06:33, 583.23it/s]\n",
      "  1%|▏                                  | 1606/231164 [00:02<06:11, 617.69it/s]\n",
      "  1%|▎                                  | 1745/231164 [00:02<06:01, 634.08it/s]\n",
      "  1%|▎                                  | 1869/231164 [00:02<05:54, 647.61it/s]\n",
      "  1%|▎                                  | 1983/231164 [00:03<05:49, 655.54it/s]\n",
      "  1%|▎                                  | 2088/231164 [00:03<05:51, 652.30it/s]\n",
      "  1%|▎                                  | 2179/231164 [00:03<05:48, 656.72it/s]\n",
      "  1%|▎                                  | 2303/231164 [00:03<05:39, 673.19it/s]\n",
      "  1%|▍                                  | 2488/231164 [00:03<05:23, 705.82it/s]\n",
      "  1%|▍                                  | 2610/231164 [00:03<05:18, 717.62it/s]\n",
      "  1%|▍                                  | 2743/231164 [00:03<05:11, 733.81it/s]\n",
      "  1%|▍                                  | 2946/231164 [00:03<04:57, 767.39it/s]\n",
      "  1%|▍                                  | 3112/231164 [00:03<04:48, 789.85it/s]\n",
      "  1%|▍                                  | 3287/231164 [00:04<04:40, 813.61it/s]\n",
      "  1%|▌                                  | 3446/231164 [00:04<04:37, 821.45it/s]\n",
      "  2%|▌                                  | 3605/231164 [00:04<04:31, 839.15it/s]\n",
      "  2%|▌                                  | 3857/231164 [00:04<04:19, 876.99it/s]\n",
      "  2%|▌                                  | 4034/231164 [00:04<04:14, 891.88it/s]\n",
      "  2%|▋                                  | 4201/231164 [00:04<04:09, 908.13it/s]\n",
      "  2%|▋                                  | 4367/231164 [00:04<04:08, 914.36it/s]\n",
      "  2%|▋                                  | 4548/231164 [00:04<04:03, 932.54it/s]\n",
      "  2%|▋                                 | 5018/231164 [00:04<03:44, 1007.63it/s]\n",
      "  2%|▊                                 | 5266/231164 [00:05<03:39, 1028.31it/s]\n",
      "  2%|▊                                 | 5493/231164 [00:05<03:37, 1039.36it/s]\n",
      "  2%|▊                                 | 5693/231164 [00:05<03:33, 1056.41it/s]\n",
      "  3%|▊                                 | 5891/231164 [00:05<03:30, 1072.65it/s]\n",
      "  3%|▉                                 | 6088/231164 [00:05<03:27, 1085.01it/s]\n",
      "  3%|▉                                 | 6276/231164 [00:05<03:25, 1094.71it/s]\n",
      "  3%|▉                                 | 6453/231164 [00:05<03:23, 1102.89it/s]\n",
      "  3%|▉                                 | 6637/231164 [00:05<03:21, 1115.46it/s]\n",
      "  3%|█                                 | 6861/231164 [00:06<03:17, 1133.86it/s]\n",
      "  3%|█                                 | 7086/231164 [00:06<03:14, 1152.19it/s]\n",
      "  3%|█                                 | 7286/231164 [00:06<03:13, 1158.71it/s]\n",
      "  3%|█                                 | 7517/231164 [00:06<03:10, 1176.92it/s]\n",
      "  3%|█▏                                | 7715/231164 [00:06<03:07, 1188.57it/s]\n",
      "  3%|█▏                                | 7912/231164 [00:06<03:06, 1195.89it/s]\n",
      "  4%|█▏                                | 8097/231164 [00:06<03:05, 1199.56it/s]\n",
      "  4%|█▏                                | 8268/231164 [00:06<03:05, 1203.67it/s]\n",
      "  4%|█▎                                | 8675/231164 [00:06<02:58, 1244.62it/s]\n",
      "  4%|█▎                                | 8912/231164 [00:07<02:56, 1257.69it/s]\n",
      "  4%|█▎                                | 9139/231164 [00:07<02:55, 1264.91it/s]\n",
      "  4%|█▎                                | 9346/231164 [00:07<02:54, 1269.15it/s]\n",
      "  4%|█▍                                | 9537/231164 [00:07<02:53, 1276.54it/s]\n",
      "  4%|█▍                                | 9723/231164 [00:07<02:52, 1282.04it/s]\n",
      "  4%|█▍                                | 9909/231164 [00:07<02:51, 1288.56it/s]\n",
      "  4%|█▍                               | 10095/231164 [00:07<02:50, 1294.40it/s]\n",
      "  4%|█▍                               | 10276/231164 [00:07<02:49, 1299.77it/s]\n",
      "  5%|█▌                               | 10792/231164 [00:08<02:43, 1347.65it/s]\n",
      "  5%|█▌                               | 11066/231164 [00:08<02:42, 1356.13it/s]\n",
      "  5%|█▌                               | 11312/231164 [00:08<02:41, 1365.52it/s]\n",
      "  5%|█▋                               | 11543/231164 [00:08<02:39, 1376.46it/s]\n",
      "  5%|█▋                               | 11774/231164 [00:08<02:38, 1384.04it/s]\n",
      "  5%|█▋                               | 12014/231164 [00:08<02:37, 1395.68it/s]\n",
      "  5%|█▊                               | 12313/231164 [00:08<02:34, 1413.99it/s]\n",
      "  5%|█▊                               | 12588/231164 [00:08<02:32, 1429.15it/s]\n",
      "  6%|█▊                               | 12843/231164 [00:08<02:31, 1438.19it/s]\n",
      "  6%|█▊                               | 13085/231164 [00:09<02:31, 1439.49it/s]\n",
      "  6%|█▉                               | 13300/231164 [00:09<02:31, 1440.64it/s]\n",
      "  6%|█▉                               | 13496/231164 [00:09<02:30, 1445.59it/s]\n",
      "  6%|█▉                               | 13942/231164 [00:09<02:27, 1476.59it/s]\n",
      " 89%|████████████████████████████▎   | 204976/231164 [01:42<00:13, 2005.42it/s]"
     ]
    }
   ],
   "source": [
    "from scipy.special import digamma, loggamma, gammaln\n",
    "\n",
    "def elbo(data, t, f_vocab, alpha, lambdas):\n",
    "    ll = log_likelihood(data, t)\n",
    "    elbo = ll\n",
    "    b = gammaln(alpha) * len(f_vocab)\n",
    "    c = gammaln(alpha * len(f_vocab))\n",
    "    for e in tqdm(t):\n",
    "        a = sum([t[e][f] * (alpha - lambdas[e][f]) + gammaln(lambdas[e][f]) - gammaln(alpha)\n",
    "                 for f in f_vocab])\n",
    "        d = gammaln(sum([lambdas[e][f] for f in f_vocab]))\n",
    "        elbo += a - b + c - c\n",
    "    return elbo\n",
    "\n",
    "def VB_IBM1(data, validation, alpha, max_steps=20, translate_dict=None):\n",
    "    print(\"Initializing translation dictionary.\")\n",
    "    if translate_dict is None:\n",
    "        translate_dict = initialize_t(data)\n",
    "    e_vocab = translate_dict.keys()\n",
    "    f_vocab = {f for e in translate_dict for f in translate_dict[e]}\n",
    "    for iteration in range(max_steps):\n",
    "        change = False\n",
    "        fname = 'iteration' + str(iteration) + '.txt'\n",
    "        lambdas = defaultdict(lambda : defaultdict(lambda : 0.05))\n",
    "\n",
    "        print(\"Expectation step {}\".format(iteration + 1))\n",
    "        for e_s, f_s in tqdm(data):\n",
    "            for f in f_s:\n",
    "                sum_of_probs = sum([translate_dict[e2][f] for e2 in e_s])\n",
    "                for e in e_s:\n",
    "                    lambdas[e][f] += translate_dict[e][f] / sum_of_probs\n",
    "\n",
    "        print(\"Maximisation step {}\".format(iteration + 1))\n",
    "        for e in tqdm(e_vocab):\n",
    "            summation = digamma(sum(lambdas[e][f2] for f2 in lambdas[e]))\n",
    "            standard = np.exp(digamma(alpha) - summation)\n",
    "            for f in f_vocab:\n",
    "                if lambdas[e][f] == alpha:\n",
    "                    translate_dict[e][f] = standard\n",
    "                else:\n",
    "                    translate_dict[e][f] = np.exp(digamma(lambdas[e][f]) - summation)\n",
    "#                 if abs(translate_dict[e][f] - new_value) > 1e-5:\n",
    "#                     change = True\n",
    "\n",
    "\n",
    "#         if not change:\n",
    "#             print(\"The translation probabilities did not change, so the model converged.\")\n",
    "#             break\n",
    "\n",
    "        # Writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, fname)\n",
    "        eb = elbo(data, translate_dict, f_vocab, alpha, lambdas)\n",
    "        print(\"Elbo: {}\".format(eb))\n",
    "        aer = test(\"\", alignments)\n",
    "        print(\"AER: {}\".format(aer))\n",
    "    return translate_dict\n",
    "\n",
    "translate_dict = VB_IBM1(training_data, validation_data, 0.05, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_IBM2(data, validation, initial_translation_estimate, max_steps=3):\n",
    "    translate_dict = initial_translation_estimate\n",
    "    jump_dict = {}\n",
    "    for iteration in range(max_steps):\n",
    "        fname = 'IBM2_iteration' + str(iteration) + '.txt'\n",
    "        counts = defaultdict(int)\n",
    "        co_counts = defaultdict(int)\n",
    "        jump_counts = defaultdict(int)\n",
    "        pos_counts = 0\n",
    "        for e_s,f_s in tqdm(data):\n",
    "            m = len(f_s)\n",
    "            l = len(e_s)\n",
    "            for i, f in enumerate(f_s):\n",
    "                sum_of_probs = 0\n",
    "                for j, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 0.1)\n",
    "                    translate_prob = translate_dict.get((e,f), 0.1)\n",
    "                    sum_of_probs += jump_prob * translate_prob\n",
    "                for j, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 0.1)\n",
    "                    translate_prob = translate_dict.get((e,f), 0.1)\n",
    "                    prob = jump_prob * translate_prob / sum_of_probs\n",
    "                    co_counts[(e,f)] += prob\n",
    "                    counts[e] += prob\n",
    "                    jump_counts[get_jump(j,i,l,m)] += prob\n",
    "                    pos_counts += prob\n",
    "        for e,f in co_counts:\n",
    "            translate_dict[(e,f)] = co_counts[(e, f)] / counts[e]\n",
    "        for jump in jump_counts:\n",
    "            jump_dict[jump] = jump_counts[jump] / pos_counts\n",
    "            \n",
    "        #writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, fname)\n",
    "        ll, ll2 = log_likelihood(data, translate_dict)\n",
    "        print(ll, ll2)\n",
    "        test(\"\", alignments)\n",
    "    return translate_dict, jump_dict\n",
    "\n",
    "def get_jump(eng_pos, fre_pos, eng_len, fre_len):\n",
    "    equivalent_pos = round(fre_pos * eng_len / fre_len)\n",
    "    return eng_pos - equivalent_pos\n",
    "\n",
    "# ibm2_transdict, ibm2_jumpdict = EM_IBM2(english_sentences, french_sentences, translate_dict, 2)\n",
    "ibm2_transdict, ibm2_jumpdict = EM_IBM2(training_data, validation_data, translate_dict, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_translation_probs('agriculture', ibm2_transdict)\n",
    "jumps = np.array(sorted(ibm2_jumpdict.items()))\n",
    "plt.plot(jumps[65:85,0], jumps[65:85,1])\n",
    "plt.show()\n",
    "print(sum(jumps[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = defaultdict(lambda : defaultdict(lambda : 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[\"hi\"][\"bonjour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
