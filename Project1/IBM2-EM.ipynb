{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 2, Expectation Maximisation\n",
    "#### Authors: Adriaan de Vries, Féliciën Veldema, Verna Dankers\n",
    "\n",
    "This notebook implements the expectation maximisation training algorithm for IBM Model 2. Run the cells in order to run the algorithm.\n",
    "\n",
    "### 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python libraries to install\n",
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "from scipy import special\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Custom imports\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from data import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the data\n",
    "\n",
    "Please set the paths to the data and run the code below. Functions for reading in the data have been placed outside of the notebook, as they are re-used by other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the -UNK- token to the data.\n",
      "English data complete.\n",
      "French data complete\n",
      "Adding the -UNK- token to the data.\n",
      "English data complete.\n",
      "French data complete\n"
     ]
    }
   ],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "# Read training data and replace words occurring once by -UNK-\n",
    "training_data = read_data(english_train, french_train, True)\n",
    "ext_data = list(zip(*training_data))\n",
    "\n",
    "# Replace words in validation data that do not appear in the training data\n",
    "validation_data = read_data(english_val, french_val, True, ttype='validation', eng_data=ext_data[0], fre_data=ext_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation of IBM2 EM\n",
    "\n",
    "First, we implement the training algorithm, and the functions to calculate alignments and log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_all(data, translate_dict, jump_dict, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = align(english_words, french_words, translate_dict, jump_dict, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "    \n",
    "def align(english_words, french_words, translate_dict, jump_dict, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict and fword in translate_dict[eword]:\n",
    "                prob = translate_dict[eword][fword] * jump_dict[get_jump(j, i, len(english_words), len(french_words))]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "        # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    co_counts = defaultdict(Counter)\n",
    "    for e, f in data:\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    co_counts[e_word][f_word] = 1\n",
    "                else:\n",
    "                    co_counts[e_word][f_word] = random()\n",
    "    for e_word in co_counts:\n",
    "        normalization_factor = sum(list(co_counts[e_word].values()))\n",
    "        for f_word in co_counts[e_word]:\n",
    "            co_counts[e_word][f_word] = co_counts[e_word][f_word] / normalization_factor\n",
    "    return co_counts\n",
    "\n",
    "def log_likelihood(data, translate_dict, jump_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = align(e, f, translate_dict, jump_dict, True)\n",
    "        logprob = 0\n",
    "        for i, j in alignment:\n",
    "            logprob += math.log(translate_dict[e[i]][f[j-1]] * jump_dict[get_jump(j-1, i, len(e), len(f))])\n",
    "        log_likelihood += logprob\n",
    "    return log_likelihood\n",
    "\n",
    "def test(path, personal_sets=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (str): path with naacl alignments\n",
    "        personal_sets: alignments in set repersesntation\n",
    "    \n",
    "    Returns:\n",
    "        int: AER\n",
    "    \"\"\"\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments('validation/dev.wa.nonullalign')\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    return metric.aer()\n",
    "\n",
    "def EM_IBM2(data, validation, initial_translation_estimate=None, initialise_method = 'IBM1', max_steps=3):\n",
    "    \"\"\"Train IBM 2 using expectation maximisation.\n",
    "    \n",
    "    Args:\n",
    "        training_data: list of tuples with parallel sentences\n",
    "        validation: list of tuples with parallel sentences\n",
    "        alpha: dirichlet prior parametrised\n",
    "        initial_translation_estimate: translation_dict initialiation\n",
    "        initialise_method: how to initialise translation probabilities\n",
    "        max_steps: number of learning iterations\n",
    "\n",
    "    Returns:\n",
    "        dict: translation probabilities\n",
    "        dict: jump probabilities\n",
    "    \"\"\"\n",
    "    initialise_method = initialise_method.lower()\n",
    "    assert initialise_method in ['ibm1', 'uniform', 'random'], \"initialise method has to be in [ibm1, uniform, random]\"\n",
    "    if initialise_method == 'ibm1':\n",
    "        assert initial_translation_estimate, \"initial_translation_method has to be given, if ibm1\"\n",
    "        translate_dict = initial_translation_estimate\n",
    "    elif initialise_method == 'uniform':\n",
    "        translate_dict = initialize_t(data, True)\n",
    "    else:\n",
    "        translate_dict = initialize_t(data, False)\n",
    "    \n",
    "    jump_dict = {}\n",
    "    for i in range(-100, 101):\n",
    "        jump_dict[i] = 1/201\n",
    "    aers = []\n",
    "    lls = []\n",
    "\n",
    "    for iteration in range(max_steps):\n",
    "        fname = 'IBM2_iteration' + str(iteration) + '.txt'\n",
    "        counts = defaultdict(int)\n",
    "        co_counts = defaultdict(int)\n",
    "        jump_counts = defaultdict(int)\n",
    "        pos_counts = 0\n",
    "        \n",
    "        print(\"Expectation step {}\".format(iteration))\n",
    "        for e_s,f_s in tqdm(data):\n",
    "            m = len(f_s)\n",
    "            l = len(e_s)\n",
    "            for j, f in enumerate(f_s):\n",
    "                sum_of_probs = 0\n",
    "                for i, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict[e][f]\n",
    "                    sum_of_probs += jump_prob * translate_prob\n",
    "                for i, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict[e][f]\n",
    "                    prob = jump_prob * translate_prob / sum_of_probs\n",
    "                    co_counts[(e,f)] += prob\n",
    "                    counts[e] += prob\n",
    "                    jump_counts[get_jump(j,i,l,m)] += prob\n",
    "                    pos_counts += prob\n",
    "\n",
    "        print(\"Maximisation step {}\".format(iteration))\n",
    "        for e, f in co_counts:\n",
    "            translate_dict[e][f] = co_counts[(e, f)] / counts[e]\n",
    "        for jump in jump_counts:\n",
    "            jump_dict[jump] = jump_counts[jump] / pos_counts\n",
    "            \n",
    "        #writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, jump_dict, fname)\n",
    "        ll = log_likelihood(data, translate_dict, jump_dict)\n",
    "        aer = test(\"\", alignments)\n",
    "        aers.append(aer)\n",
    "        lls.append(ll)\n",
    "        print(\"AER {}, LL {}\".format(aer, ll))\n",
    "        \n",
    "        # save models in between\n",
    "        pickle.dump(translate_dict, open(\"translate_dicts/ibm2_em_t_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "        pickle.dump(jump_dict, open(\"translate_dicts/ibm2_em_jump_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "    \n",
    "    # write aers and lls to file\n",
    "    with open(\"ibm2_em_scores.txt\", 'w') as f:\n",
    "        f.write(\"AER\")\n",
    "        for aer in aers:\n",
    "            f.write(\"{}\\n\".format(aer))\n",
    "        f.write(\"\\nLL\")\n",
    "        for ll in lls:\n",
    "            f.write(\"{}\\n\".format(ll))\n",
    "    return translate_dict, jump_dict\n",
    "\n",
    "def get_jump(fre_pos, eng_pos, eng_len, fre_len):\n",
    "    \"\"\"Get jump.\n",
    "    \n",
    "    Args:\n",
    "        fre_pos (int): position in French sentence\n",
    "        eng_post (int): position in English sentence\n",
    "        eng_len (int): length of English sentence\n",
    "        fre_len (int): length of French sentence\n",
    "\n",
    "    Returns:\n",
    "        int: jump\n",
    "    \"\"\"\n",
    "    equivalent_pos = int(math.floor(fre_pos * eng_len / fre_len))\n",
    "    return eng_pos - equivalent_pos\n",
    "\n",
    "# pretrained_t = pickle.load(open(\"epoch_12.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:03<00:00, 299.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 0\n",
      "AER 0.44572025052192066, LL -113561.99746145695\n",
      "Expectation step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:03<00:00, 317.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 1\n",
      "AER 0.39263157894736844, LL -85950.25559685814\n",
      "Expectation step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:02<00:00, 338.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 2\n",
      "AER 0.3947089947089947, LL -67251.42149508152\n",
      "Expectation step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:03<00:00, 333.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 3\n",
      "AER 0.38829787234042556, LL -57855.63866719885\n",
      "Expectation step 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:03<00:00, 304.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 4\n",
      "AER 0.3936170212765957, LL -53506.31695883488\n",
      "Expectation step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:02<00:00, 341.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 5\n",
      "AER 0.3978723404255319, LL -51243.424232452206\n",
      "Expectation step 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:02<00:00, 453.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 6\n",
      "AER 0.3931623931623932, LL -49909.074059091494\n",
      "Expectation step 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:02<00:00, 337.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 7\n",
      "AER 0.4027777777777778, LL -49048.16477408836\n",
      "Expectation step 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:03<00:00, 322.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 8\n",
      "AER 0.4027777777777778, LL -48470.21680337041\n",
      "Expectation step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:02<00:00, 388.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 9\n",
      "AER 0.3995726495726496, LL -48068.76398049057\n"
     ]
    }
   ],
   "source": [
    "ibm2_transdict, ibm2_jumpdict = EM_IBM2(\n",
    "    training_data[:1000], validation_data,\n",
    "    initialise_method = 'uniform', max_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
