{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from random import random\n",
    "from scipy import special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "def add_unk(original_sentences, threshold, ext_sentences=None):\n",
    "    #use original sentences and unk low frequency words\n",
    "    if ext_sentences is None: \n",
    "        counts = Counter([item for sublist in original_sentences for item in sublist])\n",
    "        unk_words = list({k:counts[k] for k in counts if counts[k] <= threshold})\n",
    "    #use external sentences and unk unknown words\n",
    "    else:\n",
    "        counts = Counter([item for sublist in ext_sentences for item in sublist])\n",
    "        unk_words = list({k:counts[k] for k in counts})\n",
    "    for line, sentence in enumerate(tqdm(original_sentences)):\n",
    "        for index, word in enumerate(sentence):\n",
    "            if word in unk_words and ext_sentences is None:\n",
    "                original_sentences[line][index] = '-UNK-'\n",
    "            elif word not in unk_words and ext_sentences is not None:\n",
    "                original_sentences[line][index] = '-UNK-'\n",
    "    return original_sentences\n",
    "    \n",
    "\n",
    "def read_data(english_file, french_file, unk=False, threshold=1, ttype='training', eng_data=None, fre_data=None):\n",
    "    english_sentences = []\n",
    "    french_sentences = []\n",
    "    with open(english_file, 'r', encoding='utf8') as engf, open(french_file, 'r', encoding='utf8') as fref:\n",
    "        for line in engf:\n",
    "            english_sentences.append([\"NULL\"] + line.split())\n",
    "        for line in fref:\n",
    "            french_sentences.append(line.split())\n",
    "    \n",
    "    #unk cases\n",
    "    if unk:\n",
    "        print(\"Adding the -UNK- token to the data.\")\n",
    "        englishname = ttype +'_'+ str(threshold)+'_unk.e'\n",
    "        frenchname = ttype +'_'+ str(threshold)+'_unk.f'\n",
    "        #load if file is found\n",
    "        if os.path.isfile(englishname):\n",
    "            with open (englishname, 'rb') as eng:\n",
    "                english_sentences = pickle.load(eng)\n",
    "        else:\n",
    "            english_sentences = add_unk(english_sentences, threshold, eng_data)\n",
    "            with open(englishname, 'wb') as eng:\n",
    "                pickle.dump(english_sentences, eng)\n",
    "        print(\"English data complete.\")\n",
    "        \n",
    "        \n",
    "        if os.path.isfile(frenchname):\n",
    "            with open(frenchname, 'rb') as fre:\n",
    "                french_sentences = pickle.load(fre)\n",
    "        else:\n",
    "            french_sentences = add_unk(french_sentences, threshold, fre_data)\n",
    "            with open(frenchname, 'wb') as fre:\n",
    "                pickle.dump(french_sentences, fre)             \n",
    "        print(\"French data complete\")          \n",
    "        \n",
    "    assert len(english_sentences) == len(french_sentences), 'data mismatch'\n",
    "    return list(zip(english_sentences, french_sentences))\n",
    "\n",
    "training_data = read_data(english_train, french_train, True)\n",
    "ext_data = list(zip(*training_data))\n",
    "validation_data = read_data(english_val, french_val, True, ttype='validation', eng_data=ext_data[0], fre_data=ext_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AER Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(path, personal_sets=None):\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments('validation/dev.wa.nonullalign')\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    return metric.aer()\n",
    "\n",
    "#hardcoded AER test for 5 iterations\n",
    "# for i in range(10):\n",
    "#     test('iteration'+str(i)+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_all(data, translate_dict, jump_dict, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = align(english_words, french_words, translate_dict, jump_dict, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "    \n",
    "def align(english_words, french_words, translate_dict, jump_dict, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict and fword in translate_dict[eword]:\n",
    "                prob = translate_dict[eword][fword] * jump_dict[get_jump(j, i, len(english_words), len(french_words))]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "        # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    co_counts = defaultdict(Counter)\n",
    "    for e, f in data:\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    co_counts[e_word][f_word] = 1\n",
    "                else:\n",
    "                    co_counts[e_word][f_word] = random()\n",
    "    for e_word in co_counts:\n",
    "        normalization_factor = sum(list(co_counts[e_word].values()))\n",
    "        for f_word in co_counts[e_word]:\n",
    "            co_counts[e_word][f_word] = co_counts[e_word][f_word] / normalization_factor\n",
    "    return co_counts\n",
    "\n",
    "def log_likelihood(data, translate_dict, jump_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = align(e, f, translate_dict, jump_dict, True)\n",
    "        logprob = 0\n",
    "        for i, j in alignment:\n",
    "            logprob += math.log(translate_dict[e[i]][f[j-1]] * jump_dict[get_jump(j-1, i, len(e), len(f))])\n",
    "        log_likelihood += logprob\n",
    "    return log_likelihood\n",
    "\n",
    "def EM_IBM2(data, validation, initial_translation_estimate=None, initialise_method = 'IBM1', max_steps=3):\n",
    "    initialise_method = initialise_method.lower()\n",
    "    assert initialise_method in ['ibm1', 'uniform', 'random'], \"initialise method has to be in [ibm1, uniform, random]\"\n",
    "    if initialise_method == 'ibm1':\n",
    "        assert initial_translation_estimate, \"initial_translation_method has to be given, if ibm1\"\n",
    "        translate_dict = initial_translation_estimate\n",
    "    elif initialise_method == 'uniform':\n",
    "        translate_dict = initialize_t(data, True)\n",
    "    else:\n",
    "        translate_dict = initialize_t(data, False)\n",
    "    \n",
    "    jump_dict = {}\n",
    "    for i in range(-100, 101):\n",
    "        jump_dict[i] = 1/201\n",
    "    aers = []\n",
    "    lls = []\n",
    "\n",
    "    for iteration in range(max_steps):\n",
    "        fname = 'IBM2_iteration' + str(iteration) + '.txt'\n",
    "        counts = defaultdict(int)\n",
    "        co_counts = defaultdict(int)\n",
    "        jump_counts = defaultdict(int)\n",
    "        pos_counts = 0\n",
    "        \n",
    "        print(\"Expectation step {}\".format(iteration))\n",
    "        for e_s,f_s in tqdm(data):\n",
    "            m = len(f_s)\n",
    "            l = len(e_s)\n",
    "            for j, f in enumerate(f_s):\n",
    "                sum_of_probs = 0\n",
    "                for i, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict[e][f]\n",
    "                    sum_of_probs += jump_prob * translate_prob\n",
    "                for i, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict[e][f]\n",
    "                    prob = jump_prob * translate_prob / sum_of_probs\n",
    "                    co_counts[(e,f)] += prob\n",
    "                    counts[e] += prob\n",
    "                    jump_counts[get_jump(j,i,l,m)] += prob\n",
    "                    pos_counts += prob\n",
    "\n",
    "        print(\"Maximisation step {}\".format(iteration))\n",
    "        for e, f in co_counts:\n",
    "            translate_dict[e][f] = co_counts[(e, f)] / counts[e]\n",
    "        for jump in jump_counts:\n",
    "            jump_dict[jump] = jump_counts[jump] / pos_counts\n",
    "            \n",
    "        #writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, jump_dict, fname)\n",
    "        ll = log_likelihood(data, translate_dict, jump_dict)\n",
    "        aer = test(\"\", alignments)\n",
    "        aers.append(aer)\n",
    "        lls.append(ll)\n",
    "        print(\"AER {}, LL {}\".format(aer, ll))\n",
    "        \n",
    "        # save models in between\n",
    "        pickle.dump(translate_dict, open(\"translate_dicts/ibm2_em_t_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "        pickle.dump(jump_dict, open(\"translate_dicts/ibm2_em_jump_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "    \n",
    "    # write aers and lls to file\n",
    "    with open(\"ibm2_em_scores.txt\", 'w') as f:\n",
    "        f.write(\"AER\")\n",
    "        for aer in aers:\n",
    "            f.write(\"{}\\n\".format(aer))\n",
    "        f.write(\"\\nLL\")\n",
    "        for ll in lls:\n",
    "            f.write(\"{}\\n\".format(ll))\n",
    "    return translate_dict, jump_dict\n",
    "\n",
    "def get_jump(fre_pos, eng_pos, eng_len, fre_len):\n",
    "    equivalent_pos = int(math.floor(fre_pos * eng_len / fre_len))\n",
    "    return eng_pos - equivalent_pos\n",
    "\n",
    "ibm2_transdict, ibm2_jumpdict = EM_IBM2(training_data[:100], validation_data, initialise_method = 'random', max_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
