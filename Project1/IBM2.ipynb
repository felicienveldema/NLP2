{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "from random import random\n",
    "from scipy import special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "def read_data(english_file, french_file):\n",
    "    english_sentences = []\n",
    "    french_sentences = []\n",
    "    with open(english_file, 'r', encoding='utf8') as engf, open(french_file, 'r', encoding='utf8') as fref:\n",
    "        for line in engf:\n",
    "            english_sentences.append([\"NULL\"] + line.split())\n",
    "        for line in fref:\n",
    "            french_sentences.append(line.split())\n",
    "    assert len(english_sentences) == len(french_sentences), 'data mismatch'\n",
    "    return list(zip(english_sentences, french_sentences))\n",
    "\n",
    "training_data = read_data(english_train, french_train)\n",
    "validation_data = read_data(english_val, french_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AER Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(path, personal_sets=None):\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments('validation/dev.wa.nonullalign')\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    print(metric.aer())\n",
    "    return metric.aer()\n",
    "\n",
    "#hardcoded AER test for 5 iterations\n",
    "# for i in range(10):\n",
    "#     test('iteration'+str(i)+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [11:40<00:00, 330.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7929724596391263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████▋                     | 86577/231164 [04:28<07:28, 322.36it/s]"
     ]
    }
   ],
   "source": [
    "def align_all(data, translate_dict, jump_dict, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = align(english_words, french_words, translate_dict, jump_dict, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "    \n",
    "def align(english_words, french_words, translate_dict, jump_dict, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict and fword in translate_dict[eword]:\n",
    "                prob = translate_dict[eword][fword] * jump_dict[get_jump(i, j, len(english_words), len(french_words))]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "        # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    co_counts = defaultdict(Counter)\n",
    "    for e, f in data:\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    co_counts[e_word][f_word] = 1\n",
    "                else:\n",
    "                    co_counts[e_word][f_word] = random()\n",
    "    for e_word in co_counts:\n",
    "        normalization_factor = sum(list(co_counts[e_word].values()))\n",
    "        for f_word in co_counts[e_word]:\n",
    "            co_counts[e_word][f_word] = co_counts[e_word][f_word] / normalization_factor\n",
    "    return co_counts\n",
    "\n",
    "def log_likelihood(data, translate_dict, jump_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = align(e, f, translate_dict, jump_dict, True)\n",
    "        logprob = 0\n",
    "        for j, i in alignment:\n",
    "            logprob += math.log(translate_dict[e[j]][f[i-1]] * jump_dict[get_jump(j, i-1, len(e), len(f))])\n",
    "        log_likelihood += logprob\n",
    "    return log_likelihood\n",
    "\n",
    "def EM_IBM2(data, validation, initial_translation_estimate=None, initialise_method = 'IBM1', max_steps=3):\n",
    "    initialise_method = initialise_method.lower()\n",
    "    assert initialise_method in ['ibm1', 'uniform', 'random'], \"initialise method has to be in [ibm1, uniform, random]\"\n",
    "    if initialise_method == 'ibm1':\n",
    "        assert initial_translation_estimate, \"initial_translation_method has to be given, if ibm1\"\n",
    "        translate_dict = initial_translation_estimate\n",
    "        \n",
    "    elif initialise_method == 'uniform':\n",
    "        translate_dict = initialize_t(data, True)\n",
    "    else:\n",
    "        translate_dict = initialize_t(data, False)\n",
    "    \n",
    "    jump_dict = {}\n",
    "    for i in range(-100, 101):\n",
    "        jump_dict[i] = 1/201\n",
    "\n",
    "    for iteration in range(max_steps):\n",
    "        fname = 'IBM2_iteration' + str(iteration) + '.txt'\n",
    "        counts = defaultdict(int)\n",
    "        co_counts = defaultdict(int)\n",
    "        jump_counts = defaultdict(int)\n",
    "        pos_counts = 0\n",
    "        for e_s,f_s in tqdm(data):\n",
    "            m = len(f_s)\n",
    "            l = len(e_s)\n",
    "            for i, f in enumerate(f_s):\n",
    "                sum_of_probs = 0\n",
    "                for j, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict.get((e,f), 0.1)\n",
    "                    sum_of_probs += jump_prob * translate_prob\n",
    "                for j, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict.get((e,f), 0.1)\n",
    "                    prob = jump_prob * translate_prob / sum_of_probs\n",
    "                    co_counts[(e,f)] += prob\n",
    "                    counts[e] += prob\n",
    "                    jump_counts[get_jump(j,i,l,m)] += prob\n",
    "                    pos_counts += prob\n",
    "        for e,f in co_counts:\n",
    "            translate_dict[(e,f)] = co_counts[(e, f)] / counts[e]\n",
    "        for jump in jump_counts:\n",
    "            jump_dict[jump] = jump_counts[jump] / pos_counts\n",
    "            \n",
    "        #writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, jump_dict, fname)\n",
    "#        ll = log_likelihood(data, translate_dict, jump_dict)\n",
    "#        print(ll)\n",
    "        test(\"\", alignments)\n",
    "    return translate_dict, jump_dict\n",
    "\n",
    "def get_jump(eng_pos, fre_pos, eng_len, fre_len):\n",
    "    equivalent_pos = int(math.floor(fre_pos * eng_len / fre_len))\n",
    "    return eng_pos - equivalent_pos\n",
    "\n",
    "# ibm2_transdict, ibm2_jumpdict = EM_IBM2(english_sentences, french_sentences, translate_dict, 2)\n",
    "ibm2_transdict, ibm2_jumpdict = EM_IBM2(training_data, validation_data, initialise_method = 'random', max_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
