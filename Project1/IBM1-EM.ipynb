{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 1, Expectation Maximisation\n",
    "#### Authors: Adriaan de Vries, Féliciën Veldema, Verna Dankers\n",
    "\n",
    "This notebook implements the expectation maximisation training algorithm for IBM Model 1. Run the cells in order to run the algorithm.\n",
    "\n",
    "### 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python libraries to install\n",
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "from scipy.special import digamma, loggamma, gammaln\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Custom requirements\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics, test\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the data\n",
    "\n",
    "Please set the paths to the data and run the code below. Functions for reading in the data have been placed outside of the notebook, as they are re-used by other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the -UNK- token to the data.\n",
      "English data complete.\n",
      "French data complete\n",
      "Adding the -UNK- token to the data.\n",
      "English data complete.\n",
      "French data complete\n"
     ]
    }
   ],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "training_data = data.read_data(english_train, french_train, True)\n",
    "ext_data = list(zip(*training_data))\n",
    "validation_data = data.read_data(english_val, french_val, True,\n",
    "    ttype='validation', eng_data=ext_data[0], fre_data=ext_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Implementation of IBM1 EM\n",
    "\n",
    "First, we implement the training algorithm, and the functions to calculate alignments and the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_all(data, translate_dict, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = align(english_words, french_words, translate_dict, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "    \n",
    "def align(english_words, french_words, translate_dict, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict and fword in translate_dict[eword]:\n",
    "                prob = translate_dict[eword][fword]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "        # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def log_likelihood(data, translate_dict, add_constant=False):\n",
    "    \"\"\"Calculate the log likelihood for the training data.\n",
    "\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_constant: whether to add the length normalisation constant\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = align(e, f, translate_dict, True)\n",
    "        prob = 0\n",
    "        for j, i in alignment:\n",
    "            prob += math.log(translate_dict[e[j]][f[i-1]])\n",
    "        log_likelihood += prob\n",
    "\n",
    "        # Length normalisation constant\n",
    "        if add_constant:\n",
    "            log_likelihood += -len(f) * np.log(len(e) + 1)\n",
    "    return log_likelihood\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    # Initialise random or uniform\n",
    "    t = defaultdict(Counter)\n",
    "    for e, f in tqdm(data):\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    t[e_word][f_word] = 1\n",
    "                else:\n",
    "                    t[e_word][f_word] = random()\n",
    "\n",
    "    # Normalise counts for every English word\n",
    "    for e_word in t:\n",
    "        normalization_factor = sum(list(t[e_word].values()))\n",
    "        for f_word in t[e_word]:\n",
    "            t[e_word][f_word] = t[e_word][f_word] / normalization_factor\n",
    "    return t\n",
    "\n",
    "def test(own_path, gold_path='validation/dev.wa.nonullalign', personal_sets=None):\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments(gold_path)\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(own_path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    return metric.aer()\n",
    "\n",
    "def EM_IBM1(data, validation, max_steps=20, translate_dict=None, epochs_trained=0):\n",
    "    \"\"\"Train IBM1 using the EM algorithm.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        validation: list of tuples, english and french sentences\n",
    "        max_steps: maximum number of iterations\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        epoch_trained: epochs already trained before\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    print(\"Initializing translation dictionary.\")\n",
    "    # If translate dict is not already given, initialise it\n",
    "    if translate_dict is None:\n",
    "        translate_dict = initialize_t(data)\n",
    "\n",
    "    for iteration in range(epochs_trained, epochs_trained + max_steps):\n",
    "        # Initialise counts\n",
    "        fname = \"iteration {}.txt\".format(iteration)\n",
    "        counts = Counter()\n",
    "        co_counts = defaultdict(Counter)\n",
    "\n",
    "        # Expectation\n",
    "        print(\"Expectation step {}\".format(iteration + 1))\n",
    "        for e_s, f_s in tqdm(data):\n",
    "            for f in f_s:\n",
    "                sum_of_probs = sum([translate_dict[e2][f] for e2 in e_s])\n",
    "                for e in e_s:\n",
    "                    delta = translate_dict[e][f] / sum_of_probs\n",
    "                    co_counts[e][f] += delta\n",
    "                    counts[e] += delta\n",
    "\n",
    "        # Maximisation\n",
    "        print(\"Maximisation step {}\".format(iteration + 1))\n",
    "        for e in co_counts:\n",
    "            for f in co_counts[e]:\n",
    "                new_value = co_counts[e][f] / counts[e]\n",
    "                translate_dict[e][f] = new_value\n",
    "\n",
    "        # Writing the iteration files in naacl for AER use\n",
    "        alignments = align_all(validation, translate_dict, fname)\n",
    "        ll = log_likelihood(data, translate_dict)\n",
    "        aer = test(\"\", personal_sets=alignments)\n",
    "        print(\"Log likelihood: {}, AER: {}\".format(ll, aer))\n",
    "\n",
    "        # Save translate_dict for later use\n",
    "        pickle.dump(translate_dict, open(\"translate_dicts/ibm1_em_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "    return translate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing translation dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 50000/50000 [00:15<00:00, 3272.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [01:16<00:00, 654.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 1\n",
      "Log likelihood: -3620590.13330799, AER: 0.40835707502374174\n",
      "Expectation step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [01:18<00:00, 637.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 2\n",
      "Log likelihood: -2626810.406258484, AER: 0.34574976122254064\n",
      "Expectation step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [01:24<00:00, 588.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 3\n",
      "Log likelihood: -2173920.021614505, AER: 0.34416826003824097\n",
      "Expectation step 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [01:18<00:00, 639.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 4\n",
      "Log likelihood: -1954758.4640658444, AER: 0.33747609942638623\n",
      "Expectation step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [01:17<00:00, 643.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 5\n",
      "Log likelihood: -1826981.2982877223, AER: 0.3413001912045889\n"
     ]
    }
   ],
   "source": [
    "translate_dict_em = EM_IBM1(training_data[:50000], validation_data, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
