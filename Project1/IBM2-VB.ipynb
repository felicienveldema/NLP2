{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 2, Variational Bayes\n",
    "#### Authors: Adriaan de Vries, Féliciën Veldema, Verna Dankers\n",
    "\n",
    "This notebook implements the variational bayes training algorithm for IBM Model 2. Run the cells in order to run the algorithm.\n",
    "\n",
    "### 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python libraries to install\n",
    "from __future__ import print_function, division\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "from scipy import special\n",
    "from scipy.special import digamma, loggamma, gammaln\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Custom imports\n",
    "from aer import read_naacl_alignments, AERSufficientStatistics\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in the data\n",
    "\n",
    "Please set the paths to the data and run the code below. Functions for reading in the data have been placed outside of the notebook, as they are re-used by other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the -UNK- token to the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 603/231164 [00:16<1:47:08, 35.86it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-866862f2b338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Read training data and replace words occurring once by -UNK-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrench_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mext_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wietze\\Documents\\Verna\\NLP2\\Project1\\data.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(english_file, french_file, unk, threshold, ttype, eng_data, fre_data)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0menglish_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0menglish_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_unk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglishname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglish_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Wietze\\Documents\\Verna\\NLP2\\Project1\\data.py\u001b[0m in \u001b[0;36madd_unk\u001b[1;34m(original_sentences, threshold, ext_sentences)\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0moriginal_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'-UNK-'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munk_words\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mext_sentences\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0moriginal_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'-UNK-'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moriginal_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                   | 603/231164 [00:30<3:11:13, 20.10it/s]"
     ]
    }
   ],
   "source": [
    "english_train = 'training/hansards.36.2.e'\n",
    "french_train = 'training/hansards.36.2.f'\n",
    "english_val = 'validation/dev.e'\n",
    "french_val = 'validation/dev.f'\n",
    "fname = 'naacltest.txt'\n",
    "\n",
    "# Read training data and replace words occurring once by -UNK-\n",
    "training_data = data.read_data(english_train, french_train, True)\n",
    "ext_data = list(zip(*training_data))\n",
    "\n",
    "# Replace words in validation data that do not appear in the training data\n",
    "validation_data = data.read_data(english_val, french_val, True, ttype='validation', eng_data=ext_data[0], fre_data=ext_data[1])\n",
    "test_data = data.read_data(\"testing/test/test.e\", \"testing/test/test.f\", True, ttype='test', eng_data=ext_data[0], fre_data=ext_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation of IBM2 VB\n",
    "\n",
    "First, we implement the training algorithm, and the functions to calculate alignments, log likelihood and elbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbo(data, t, jumps, f_vocab, alpha, lambdas):\n",
    "    \"\"\"Calculate the ELBO for the training data.\n",
    "\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        t: dictionary with translation probabilities e to f\n",
    "        jumps: alignment probabilities\n",
    "        f_vocab: set of French words\n",
    "        alpha: value for dirichlet prior\n",
    "        lambdas: adapted counts from last iteration\n",
    "\n",
    "    Returns:\n",
    "        float: elbo\n",
    "    \"\"\"\n",
    "    # Start by calculating log likelihood\n",
    "    ll = log_likelihood(data, t, jumps)\n",
    "    \n",
    "    # Add -KL to the log likelihood\n",
    "    elbo = ll\n",
    "    gammaln_alpha = gammaln(alpha)\n",
    "    c = gammaln(alpha * len(f_vocab))\n",
    "    for e in tqdm(t):\n",
    "        a = sum([(math.log(t[e][f]) if t[e][f] != 0 else 0) * (alpha - lambdas[e][f])\n",
    "                 +  gammaln(lambdas[e][f]) - gammaln_alpha for f in lambdas[e] if f != \"-REST-\"])\n",
    "        b = gammaln(sum([(lambdas[e][f] if f in lambdas[e] else alpha) for f in f_vocab]))\n",
    "        elbo += a - b + c\n",
    "    return elbo\n",
    "\n",
    "def log_likelihood(data, translate_dict, jump_dict):\n",
    "    \"\"\"Calculate the log likelihood for the training data.\n",
    "\n",
    "    Args:\n",
    "        data: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        jump_dict: alignment probabilities\n",
    "\n",
    "    Returns:\n",
    "        float: log likelihood\n",
    "    \"\"\"\n",
    "    log_likelihood = 0\n",
    "    for e, f in data:\n",
    "        alignment = VB_align(e, f, translate_dict, jump_dict, True)\n",
    "        logprob = 0\n",
    "        for i, j in alignment:\n",
    "            logprob += math.log(translate_dict[e[i]][f[j-1]] * jump_dict[get_jump(j-1, i, len(e), len(f))])\n",
    "        log_likelihood += logprob\n",
    "    return log_likelihood\n",
    "\n",
    "def VB_align_all(data, translate_dict, jump_dict, f_vocab, fname=None):\n",
    "    \"\"\"Create alignments for pairs of English and French sentences.\n",
    "    Both save them as sets per sentence and pair and save to file.\n",
    "    \n",
    "    Args:\n",
    "        validation: zipped object with pairs of e and f sentences\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        fname: filename to save alignments in, in NAACL format\n",
    "\n",
    "    Returns:\n",
    "        list of sets\n",
    "    \"\"\"\n",
    "    file = open(fname, 'w')\n",
    "    alignments = []\n",
    "    for k, (english_words, french_words) in enumerate(data):\n",
    "        alignment = VB_align(english_words, french_words, translate_dict, jump_dict, f_vocab, False)\n",
    "        for pos1, pos2 in alignment:\n",
    "            file.write(\"{} {} {}\\n\".format(str(k+1), str(pos1), str(pos2)))\n",
    "        alignments.append(set(alignment))\n",
    "    return alignments\n",
    "\n",
    "def VB_align(english_words, french_words, translate_dict, jump_dict, f_vocab, add_null=True):\n",
    "    \"\"\"Align one sentence pair, either with or without the NULL alignments.\n",
    "    \n",
    "    Args:\n",
    "        english_words: list of english words\n",
    "        french_words: list of french words\n",
    "        translate_dict: dictionary with translation probabilities e to f\n",
    "        add_null: boolean to indicate whether NULL alignments should be included\n",
    "\n",
    "    Return:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    alignment = []\n",
    "    for j, fword in enumerate(french_words):\n",
    "        prior = 0.0\n",
    "        alignment_j = 0\n",
    "        for i, eword in enumerate(english_words):\n",
    "            # Only include terms that are in the dictionary\n",
    "            if eword in translate_dict:\n",
    "                if fword in translate_dict[eword]:\n",
    "                    prob = translate_dict[eword][fword]\n",
    "                else:\n",
    "                    prob = translate_dict[eword][\"-REST-\"]\n",
    "                prob = prob * jump_dict[get_jump(j, i, len(english_words), len(french_words))]\n",
    "                if prob > prior:\n",
    "                    prior = prob\n",
    "                    alignment_j = i\n",
    "                # Add dependent on whether it's a NULL alignments\n",
    "        if alignment_j != 0 or add_null:\n",
    "            alignment.append((alignment_j, j + 1))\n",
    "    return alignment\n",
    "\n",
    "def initialize_t(data, uniform=True):\n",
    "    \"\"\"Initialise the translation probabilities.\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuples, english and french sentences\n",
    "        uniform: boolean indicating initialisation type\n",
    "\n",
    "    Returns:\n",
    "        defaultdict(Counter)\n",
    "    \"\"\"\n",
    "    t = defaultdict(Counter)\n",
    "    for e, f in data:\n",
    "        for e_word in e:\n",
    "            for f_word in f:\n",
    "                if uniform:\n",
    "                    t[e_word][f_word] = 1\n",
    "                else:\n",
    "                    t[e_word][f_word] = random()\n",
    "    for e_word in t:\n",
    "        normalization_factor = sum(list(t[e_word].values()))\n",
    "        for f_word in t[e_word]:\n",
    "            t[e_word][f_word] = t[e_word][f_word] / normalization_factor\n",
    "    return t\n",
    "\n",
    "def test(path, personal_sets=None):\n",
    "    \"\"\"Compute AER for alignments.\n",
    "\n",
    "    Args:\n",
    "        path (str): path with naacl alignments\n",
    "        personal_sets: alignments in set repersesntation\n",
    "    \n",
    "    Returns:\n",
    "        int: AER\n",
    "    \"\"\"\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments('validation/dev.wa.nonullalign')\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm\n",
    "    if personal_sets is None:\n",
    "        personal_sets = read_naacl_alignments(path)\n",
    "        predictions = []\n",
    "        for s, p in personal_sets:\n",
    "            links = set()\n",
    "            for link in s:\n",
    "                links.add(link)\n",
    "            predictions.append(links)\n",
    "    else:\n",
    "        predictions=personal_sets\n",
    "\n",
    "    # 3. Compute AER\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    return metric.aer()\n",
    "\n",
    "def VB_IBM2(training_data, validation, alpha, initial_translation_estimate=None, initialise_method = 'IBM1', max_steps=3):\n",
    "    \"\"\"Train IBM 2 using variational inference.\n",
    "    \n",
    "    Args:\n",
    "        training_data: list of tuples with parallel sentences\n",
    "        validation: list of tuples with parallel sentences\n",
    "        alpha: dirichlet prior parametrised\n",
    "        initial_translation_estimate: translation_dict initialiation\n",
    "        initialise_method: how to initialise translation probabilities\n",
    "        max_steps: number of learning iterations\n",
    "\n",
    "    Returns:\n",
    "        dict: translation probabilities\n",
    "        dict: jump probabilities\n",
    "    \"\"\"\n",
    "    initialise_method = initialise_method.lower()\n",
    "    assert initialise_method in ['ibm1', 'uniform', 'random'], \"initialise method has to be in [ibm1, uniform, random]\"\n",
    "    if initialise_method == 'ibm1':\n",
    "        assert initial_translation_estimate, \"initial_translation_method has to be given, if ibm1\"\n",
    "        translate_dict = initial_translation_estimate\n",
    "    elif initialise_method == 'uniform':\n",
    "        translate_dict = initialize_t(training_data, True)\n",
    "    else:\n",
    "        translate_dict = initialize_t(training_data, False)\n",
    "    \n",
    "    jump_dict = {}\n",
    "    for i in range(-100, 101):\n",
    "        jump_dict[i] = 1/201\n",
    "    aers = []\n",
    "    elbos = []\n",
    "    f_vocab = {f for e in translate_dict for f in translate_dict[e]}\n",
    "\n",
    "    for iteration in range(max_steps):\n",
    "        fname = 'IBM2_iteration' + str(iteration) + '.txt'\n",
    "        lambdas = defaultdict(lambda : defaultdict(lambda : alpha))\n",
    "        jump_counts = defaultdict(int)\n",
    "        pos_counts = 0\n",
    "        \n",
    "        # VB `expectation'\n",
    "        print(\"Expectation step {}\".format(iteration))\n",
    "        for e_s,f_s in tqdm(training_data):\n",
    "            m = len(f_s)\n",
    "            l = len(e_s)\n",
    "            for j, f in enumerate(f_s):\n",
    "                sum_of_probs = 0\n",
    "                # Calculate normalizers\n",
    "                for i, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict[e][f]\n",
    "                    sum_of_probs += jump_prob * translate_prob\n",
    "\n",
    "                # Collect counts\n",
    "                for i, e in enumerate(e_s):\n",
    "                    jump_prob = jump_dict.get(get_jump(j,i,l,m), 1/l)\n",
    "                    translate_prob = translate_dict[e][f]\n",
    "                    prob = jump_prob * translate_prob / sum_of_probs\n",
    "                    lambdas[e][f] += prob\n",
    "                    jump_counts[get_jump(j,i,l,m)] += prob\n",
    "                    pos_counts += prob\n",
    "\n",
    "        # VB `maximisation'\n",
    "        print(\"Maximisation step {}\".format(iteration))\n",
    "        for e in tqdm(translate_dict):\n",
    "            summation = 0\n",
    "            for f2 in f_vocab:\n",
    "                if f2 in lambdas[e]:\n",
    "                    summation += lambdas[e][f2]\n",
    "                else:\n",
    "                    summation += alpha\n",
    "            summation = digamma(summation)\n",
    "            for f in translate_dict[e]:\n",
    "                translate_dict[e][f] = np.exp(digamma(lambdas[e][f]) - summation)\n",
    "            translate_dict[e][\"-REST-\"] = np.exp(digamma(alpha) - summation)\n",
    "\n",
    "        for jump in jump_counts:\n",
    "            jump_dict[jump] = jump_counts[jump] / pos_counts\n",
    "            \n",
    "        # Writing the iteration files in naacl for AER use\n",
    "        alignments = VB_align_all(validation, translate_dict, jump_dict, f_vocab, fname)\n",
    "        _ = VB_align_all(test_data, translate_dict, jump_dict, f_vocab, \"ibm1_{}.vb.naacl\".format(iteration+1))\n",
    "        eb = elbo(training_data, translate_dict, jump_dict, f_vocab, alpha, lambdas)\n",
    "        aer = test(\"\", alignments)\n",
    "        aers.append(aer)\n",
    "        elbos.append(eb)\n",
    "        print(\"AER {}, LL {}\".format(aer, eb))\n",
    "        \n",
    "        # Save models in between\n",
    "        pickle.dump(translate_dict, open(\"translate_dicts/ibm2_em_t_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "        pickle.dump(jump_dict, open(\"translate_dicts/ibm2_em_jump_epoch_{}.pickle\".format(iteration + 1), 'wb'))\n",
    "    \n",
    "    # Write aers and lls to file\n",
    "    with open(\"ibm2_vb_{}_scores.txt\".format(alpha), 'w') as f:\n",
    "        f.write(\"AER\")\n",
    "        for aer in aers:\n",
    "            f.write(\"{}\\n\".format(aer))\n",
    "        f.write(\"\\nLL\")\n",
    "        for eb in elbos:\n",
    "            f.write(\"{}\\n\".format(eb))\n",
    "    return translate_dict, jump_dict\n",
    "\n",
    "def get_jump(fre_pos, eng_pos, eng_len, fre_len):\n",
    "    \"\"\"Get jump.\n",
    "    \n",
    "    Args:\n",
    "        fre_pos (int): position in French sentence\n",
    "        eng_post (int): position in English sentence\n",
    "        eng_len (int): length of English sentence\n",
    "        fre_len (int): length of French sentence\n",
    "\n",
    "    Returns:\n",
    "        int: jump\n",
    "    \"\"\"\n",
    "    equivalent_pos = int(math.floor(fre_pos * eng_len / fre_len))\n",
    "    return eng_pos - equivalent_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [11:03<00:00, 348.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25593/25593 [05:56<00:00, 71.87it/s]\n",
      "100%|████████████████████████████████████| 25593/25593 [05:48<00:00, 73.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AER 0.24431256181998018, LL -22982532.1409312\n",
      "Expectation step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [11:15<00:00, 342.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25593/25593 [07:53<00:00, 54.01it/s]\n",
      "100%|████████████████████████████████████| 25593/25593 [06:51<00:00, 62.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AER 0.24751491053677932, LL -21772031.874373805\n",
      "Expectation step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [11:49<00:00, 325.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25593/25593 [05:58<00:00, 71.47it/s]\n",
      "100%|████████████████████████████████████| 25593/25593 [07:13<00:00, 59.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AER 0.24900398406374502, LL -21499952.426671233\n",
      "Expectation step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [11:19<00:00, 340.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25593/25593 [06:12<00:00, 68.71it/s]\n",
      "100%|████████████████████████████████████| 25593/25593 [06:10<00:00, 69.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AER 0.24275724275724275, LL -21372806.558607686\n",
      "Expectation step 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [10:47<00:00, 356.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25593/25593 [06:17<00:00, 67.75it/s]\n",
      "100%|████████████████████████████████████| 25593/25593 [06:36<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AER 0.239, LL -21295252.372020103\n",
      "Expectation step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [11:12<00:00, 343.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximisation step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25593/25593 [05:58<00:00, 71.31it/s]\n",
      "100%|████████████████████████████████████| 25593/25593 [06:38<00:00, 64.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AER 0.242, LL -21244910.112500586\n",
      "Expectation step 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████▌| 228276/231164 [10:32<00:08, 360.97it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5d186de35039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m ibm2_transdict, ibm2_jumpdict = EM_IBM2(\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_translation_estimate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrained_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-8-5d186de35039>\u001b[0m in \u001b[0;36mEM_IBM2\u001b[1;34m(training_data, validation, alpha, initial_translation_estimate, initialise_method, max_steps)\u001b[0m\n\u001b[0;32m    193\u001b[0m                     \u001b[0mtranslate_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslate_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjump_prob\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtranslate_prob\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum_of_probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                     \u001b[0mlambdas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m                     \u001b[0mjump_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_jump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mpos_counts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████▌| 228276/231164 [10:50<00:08, 350.97it/s]"
     ]
    }
   ],
   "source": [
    "pretrained_t = pickle.load(open(\"translate_dicts/IBM1VB/ibm1_vb_epoch_13.pickle\", 'rb'))\n",
    "alpha = 0.0001\n",
    "ibm2_transdict, ibm2_jumpdict = VB_IBM2(\n",
    "    training_data, validation_data, alpha, initial_translation_estimate=pretrained_t, max_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
